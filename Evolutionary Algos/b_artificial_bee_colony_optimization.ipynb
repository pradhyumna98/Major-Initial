{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,timeit\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "def random_search(n,dim):\n",
    "    gens=[[0 if g != j else 1 for g in range(n)] for j in range(dim)]\n",
    "    return gens\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.KFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data):\n",
    "            s+= RandomForestClassifier(n_estimators=25).fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s\n",
    "\n",
    "def bees_optimization(bee,binary,i):\n",
    "    binary=list(binary)\n",
    "    j=random.randint(0,len(binary)-1)\n",
    "    k=random.randint(0,len(binary)-1)\n",
    "    while k==i:\n",
    "        k=random.randint(0,len(binary)-1)\n",
    "    fit=binary[j]+random.uniform(-1,1)*(binary[j]-binary[k])\n",
    "    for x in range(bee):\n",
    "        y=random.randint(0,len(binary)-1)\n",
    "        while y==i:\n",
    "            y=random.randint(0,len(binary)-1)\n",
    "        r=random.uniform(0,1)\n",
    "        if r<=fit:\n",
    "            binary[y]=1\n",
    "        \n",
    "    return binary\n",
    "\n",
    "def BABCO(train_d,train_l,n=10,max_iter=25,employee_percent=0.5,max_limit=5):\n",
    "    \"\"\"\n",
    "    input:{ Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] â†’ 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    employed_bees = int(round(n*employee_percent))\n",
    "    onlooker_bees = n - employed_bees       \n",
    "\n",
    "    dim=len(train_d[0])\n",
    "    global_best=float(\"-inf\")\n",
    "    global_position=tuple([0]*dim)\n",
    "    gens_dict = {}\n",
    "    limit=[0]*dim\n",
    "    gens=random_search(dim,dim)\n",
    "    for gen in gens:\n",
    "        if tuple(gen) in gens_dict:\n",
    "            score = gens_dict[tuple(gen)]\n",
    "        else:\n",
    "            score=evaluate(train_d,train_l,gen)\n",
    "            gens_dict[tuple(gen)]=score\n",
    "        if score > global_best:\n",
    "            global_best=score\n",
    "            global_position=dc(gen)\n",
    "            \n",
    "            \n",
    "    for it in range(max_iter):\n",
    "        for i in range(employed_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "    \n",
    "        for i in range(onlooker_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "\n",
    "                \n",
    "    return global_position,global_position.count(1)\n",
    "\n",
    "\n",
    "def test_score(gen,tr_x,tr_y,te_x,te_y):\n",
    "    mask=np.array(gen) == 1\n",
    "    al_data=np.array(tr_x[:,mask])\n",
    "    al_test_data=np.array(te_x[:,mask])\n",
    "    return np.mean([RandomForestClassifier(n_estimators=25).fit(al_data,tr_y).score(al_test_data,te_y) for i in range(5)])\n",
    "\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \"\"   \n",
    "    for ele in s:  \n",
    "        str1 += ele     \n",
    "    return str1\n",
    "\n",
    "def Bee_Colony(k,train_d,test_d,train_l,test_l)\n",
    "#     k=[1 for r in range(len(x[0]))]\n",
    "    print(test_score(k,train_d,train_l,test_d,test_l))\n",
    "    max_1 = test_score(k,train_d,train_l,test_d,test_l)\n",
    "    colo = listToString(k)\n",
    "    fattr=0\n",
    "    ftest=0.0\n",
    "    flist=[0 for r in range(len(x[0]))]\n",
    "    final_list=[0 for r in range(len(x[0]))]\n",
    "    start=timeit.default_timer()\n",
    "    for i in range(20):\n",
    "        g,l=BABCO(train_d,train_l,n=10,max_iter=25,employee_percent=0.5,max_limit=5)\n",
    "        fattr+=l\n",
    "        test=test_score(g,train_d,train_l,test_d,test_l)\n",
    "        if test>max_1:\n",
    "            max_1 = test\n",
    "            colo= \"\".join(map(str,g))\n",
    "        ftest+=test\n",
    "        for j in range(len(flist)):\n",
    "            if g[j]==1:\n",
    "                flist[j]+=1\n",
    "        print(\"{0}  {1}  {2}  {3:.6f}\".format(i+1,\"\".join(map(str,g)),l,test))\n",
    "    fattr=fattr//20\n",
    "    ftest=ftest/20\n",
    "    end=timeit.default_timer()\n",
    "    time=end-start\n",
    "    print(flist,fattr)\n",
    "    final=np.argsort(flist)[::-1][:fattr]\n",
    "    print(final)\n",
    "    for i in range(len(final)):\n",
    "        final_list[final[i]]=1\n",
    "    print(\"{0}  {1}   {2}   {3:.6f}    {4:.4f}\".format(\"Final: \",\"\".join(map(str,final_list)),fattr,ftest,time))\n",
    "    return max_1,colo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
